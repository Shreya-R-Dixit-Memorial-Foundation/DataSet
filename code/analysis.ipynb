{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do\n",
    "\n",
    "Now is a good time to look at everythign we have - here is the file location:\n",
    "\n",
    "/Users/yd211/Documents/GitHub/DataSet/DVSCrashData/MN Crash Data 2015 and Documentation/~$-Crash-Data-Code-Guide-2003-To-2015.docx\n",
    "\n",
    "\n",
    "Okay so here are somethings that we can do - some questions we can do:\n",
    "\n",
    "1. What age is the most represented, what age is the most over represented\n",
    "\n",
    "2. What ACC TIME bucket is the most prone? Challenge, waht ACC Time is the most overprone?\n",
    "\n",
    "3. What city has the most accidents?\n",
    "\n",
    "6. What percentage of accidents are given the alchool test?\n",
    "\n",
    "7. What are the most common primary secondar and tertiarry contributin factors and how represented are they (like what perecentage is the number one factor and number two factor) - could help in picking a direction for what to do to help reduce crashes the most\n",
    "\n",
    "8. What part of the car is the most damaged?\n",
    "\n",
    "9. What is the ususal crash configuration? - what disagram?\n",
    "\n",
    "10. At what kind of light do crashses usually occour, is it really true that nightime crashes are more often? Are they more dangerous? Is the speed higher at one timezone compared to other?\n",
    "\n",
    "11. Any make overrepresented in crashes?\n",
    "\n",
    "12. what's the maximum number of deaths in a crash?\n",
    "\n",
    "13. Do more alcholics crash or non alcholics?\n",
    "\n",
    "14. Suset vs Sunrise crashes and roadway direction - any changes in that?\n",
    "\n",
    "15. Any vehical types that are overrepresented?\n",
    "\n",
    "16. Any changes when snowing - WEATHER WEATH2 use that to track for snow and rain - what percentage of MN crashes occour in snow and are they more dangeorous than non snow - in terms of accident severatiy\n",
    "\n",
    "\n",
    "Other cool things that could be done reagrading map analaysis - \n",
    "\n",
    "1. Track all the accidents that occour when drinking alchool - roads that people who drink alchool usually take or is over represented\n",
    "\n",
    "2. Do analysis to find locations that have more accidents in snow than other weather events, might tell us a bit more about roads and intersections that need more work\n",
    "\n",
    "3. Could there be a mapping algorithm uses for weather (and possibly for drunk drivers)?\n",
    "\n",
    "\n",
    "Other observations - we get about 80,000 accidents each year accept 2020, where it was 50k - so total accidents that we have access to is around 450k or around half a million data points. That's a lot. We can also request data for 2022 and 2023 soon and add around 150k more accidents to our dataset. \n",
    "\n",
    "Population of MN is about 5.5 Million and has been increasing, perhaps we can do a population vs accients metric to see if the percentageg of people who get into an accident is consistent or not. Perhaps it will be. \n",
    "\n",
    "80k/5.5m = =8/550 or ~= 1.5%\n",
    "\n",
    "It would also be intresting to see how this figure compares against different states and such\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in merged data: 1983168\n",
      "Columns in the merged data: ['ACCN', 'AGENCY', 'LOCCASE', 'HITRUN', 'PROPDAM', 'NUMMV', 'NUMFAT', 'NUMINJ', 'DOLMIN', 'ACCDATE', 'COUNTY', 'CITYTWP', 'CITYNAME', 'ACCTYPE', 'SBUS', 'LOCFHE', 'BRIDGE', 'WKZNTYPE', 'LOCWKZN', 'WORKERS', 'RDSURF', 'INTREL', 'WEATHER1', 'WEATHER2', 'LIGHT', 'DIAGRAM', 'OFFTYPE', 'INTERSECT', 'ACCSEV', 'CFR1', 'CFR2', 'FATAL', 'FATWKZN', 'INJURY', 'INTYPE', 'LANDOWN', 'LEPRES', 'NUMNM', 'ONROAD', 'RTSYS', 'WKZNREL', 'CITY', 'XCOORD', 'YCOORD', 'TOWNSHIP', 'ACCTIME', 'URBRURT', 'ACCDAY', 'ACCYEAR', 'RVN_x', 'nmaction', 'unitper', 'RPN', 'POSITN', 'DLSTATE', 'DLCLASS', 'DLSTAT', 'VIOLS', 'DLREST1', 'PHYSCND1', 'RECOMND', 'SEX', 'SAFEQP1', 'AIRBAG', 'EJECT', 'INJSEV', 'ALCTEST', 'ALCTYPE', 'DRUGTEST', 'DRUGTYPE', 'METHHOSP', 'DLZIP', 'PTYPE', 'DISTRACT', 'ALCSUSP', 'NEWBAC', 'SPEEDING', 'CFP1', 'CFP2', 'CFP3', 'CFP4', 'CHARGED', 'DLENDOR1', 'DLENDOR2', 'DLENDOR3', 'DLJURIS', 'DLREST2', 'DLREST3', 'DRUGRES', 'DRUGSUSP', 'NMLOCTN', 'PHYSCND2', 'SAFEQP2', 'AGE', 'RVN_y', 'FIRE', 'TOWAWAY', 'TRAILER', 'DIRECTN', 'VEHMAKE', 'VEHMODEL', 'VEHYEAR', 'VEHCOLOR', 'REGSTATE', 'REGYEAR', 'EVENT1', 'EVENT2', 'EVENT3', 'EVENT4', 'MOSTHE', 'CFV1', 'CFV2', 'ACTION', 'TOTOCC', 'VEHTYPE', 'VEHUSE', 'DAMAREA', 'DAMSEV', 'CARGOTP', 'HAZPLAC', 'WAIVED', 'RDDES', 'RDALIGN', 'RDGRADE', 'TRFCNTL', 'WORKING', 'SPEED', 'UNITVEH', 'BUSTYPE', 'CMV', 'CMVCONFIG', 'CMVCNTRY', 'CMVSTATE', 'CMVTYPE', 'DRCONTRL', 'EMGUSE', 'GVWR', 'HAZREL', 'HITRUNV', 'INS', 'LANES', 'PARKED', 'UNITFAT', 'UNITINJ', 'VIN', 'INCIDENTID', 'Utmx', 'Utmy', 'UTMX', 'UTMY']\n",
      "Merged data saved to merged_data_2016_2017_2018_2019_2020_2021.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to load the data for a given year\n",
    "def load_data(year):\n",
    "    base_path = f\"/Users/yd211/Documents/GitHub/DataSet/DVSCrashData/MN Crash Data {year}\"\n",
    "    \n",
    "    # Define file paths\n",
    "    acc_loc = f\"{base_path}/mn-{year}-acc.txt\"\n",
    "    per_loc = f\"{base_path}/mn-{year}-per.txt\"\n",
    "    veh_loc = f\"{base_path}/mn-{year}-veh.txt\"\n",
    "    \n",
    "    # Read the data files\n",
    "    # We use low_memory=False to avoid the dtype warning for mixed types\n",
    "    accidents = pd.read_csv(acc_loc, delimiter='\\t', low_memory=False, encoding='latin1')\n",
    "    people = pd.read_csv(per_loc, delimiter='\\t', low_memory=False, encoding='latin1')\n",
    "    vehicles = pd.read_csv(veh_loc, delimiter='\\t', low_memory=False, encoding='latin1')\n",
    "    \n",
    "    return accidents, people, vehicles\n",
    "\n",
    "# Function to merge dataframes for a given year\n",
    "def merge_data(accidents, people, vehicles):\n",
    "    # Merge dataframes on 'ACCN' column\n",
    "    merged_df = pd.merge(accidents, people, on='ACCN', how='outer')\n",
    "    merged_df = pd.merge(merged_df, vehicles, on='ACCN', how='outer')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Function to load and merge data for multiple years\n",
    "def load_and_merge_years(years):\n",
    "    merged_yearly_data = []\n",
    "\n",
    "    for year in years:\n",
    "        accidents, people, vehicles = load_data(year)\n",
    "        merged_df = merge_data(accidents, people, vehicles)\n",
    "        merged_yearly_data.append(merged_df)\n",
    "    \n",
    "    # Concatenate all yearly dataframes\n",
    "    all_years_merged_df = pd.concat(merged_yearly_data, ignore_index=True)\n",
    "    return all_years_merged_df\n",
    "\n",
    "# Main function\n",
    "def main(years):\n",
    "    # Load and merge data for the given years\n",
    "    merged_df = load_and_merge_years(years)\n",
    "    \n",
    "    # Print some basic info\n",
    "    print(\"Number of rows in merged data:\", len(merged_df))\n",
    "    print(\"Columns in the merged data:\", merged_df.columns.tolist())\n",
    "\n",
    "    # Save merged data to a CSV file\n",
    "    years_str = \"_\".join(map(str, years))\n",
    "    merged_df.to_csv(f\"merged_data.csv\", index=False)\n",
    "    print(f\"Merged data saved to merged_data_{years_str}.csv\")\n",
    "\n",
    "\n",
    "years = [2016, 2017, 2018, 2019, 2020, 2021]  # Change this to the list of years you want to analyze\n",
    "main(years)\n",
    "years_str = \"_\".join(map(str, years))\n",
    "CSV_path = \"merged_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.read_csv(CSV_path, low_memory=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most represented ages:\n",
      "AGE\n",
      "18.0    18918\n",
      "17.0    18249\n",
      "19.0    17891\n",
      "22.0    17331\n",
      "23.0    17288\n",
      "24.0    17177\n",
      "21.0    17154\n",
      "20.0    17091\n",
      "16.0    16449\n",
      "25.0    16388\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter out invalid ages (e.g., 999)\n",
    "valid_ages = merged_df[merged_df['AGE'] != 999]\n",
    "\n",
    "# Get the top 10 most represented ages\n",
    "top_10_ages = valid_ages['AGE'].value_counts().head(10)\n",
    "print(\"Top 10 most represented ages:\")\n",
    "print(top_10_ages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most represented ACC TIME values:\n",
      "ACCTIME\n",
      "1700    4442\n",
      "1600    4147\n",
      "1730    3811\n",
      "1530    3622\n",
      "1500    3552\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 most represented ACC TIME values\n",
    "top_5_times = merged_df['ACCTIME'].value_counts().head(5)\n",
    "print(\"Top 5 most represented ACC TIME values:\")\n",
    "print(top_5_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The city with the most accidents is: 2585\n",
      "Top 5 cities with the most accidents:\n",
      "CITY\n",
      "2585    111612\n",
      "3425     56511\n",
      "1040     18623\n",
      "3235     18149\n",
      "3380     17632\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 3: What city has the most accidents?\n",
    "most_accidents_city = merged_df['CITY'].value_counts().idxmax()\n",
    "print(f\"The city with the most accidents is: {most_accidents_city}\")\n",
    "\n",
    "# Also, let's print the top 5 cities with the most accidents\n",
    "top_5_cities = merged_df['CITY'].value_counts().head(5)\n",
    "print(\"Top 5 cities with the most accidents:\")\n",
    "print(top_5_cities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2585= Minneapolis\n",
    "\n",
    "3425=ST PAUL\n",
    "\n",
    "1040 = DULUTH \n",
    "\n",
    "3235 = ROCHESTER\n",
    "\n",
    "3380 = ST CLOUD \n",
    "\n",
    "Intresting to see Minneapolist and St.Paul as no 1 no 2, it's validating for sure. Another intresting thing we could do is divide this number by population and see what data we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of accidents where an alcohol test was given to at least one person is: 2.37%\n"
     ]
    }
   ],
   "source": [
    "# Question 6: What percentage of accidents are given the alcohol test?\n",
    "\n",
    "# Filter out the rows where ALCTEST is relevant (i.e., rows related to people)\n",
    "people_with_alcohol_test_info = merged_df.dropna(subset=['ALCTEST'])\n",
    "\n",
    "# Calculate the percentage of people given an alcohol test\n",
    "alcohol_test_given = people_with_alcohol_test_info[people_with_alcohol_test_info['ALCTEST'] == 1]\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_alcohol_test_given = (len(alcohol_test_given) / len(people_with_alcohol_test_info)) * 100\n",
    "print(f\"The percentage of accidents where an alcohol test was given to at least one person is: {percentage_alcohol_test_given:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 primary contributing factors:\n",
      "CFP1\n",
      "1.0     388751\n",
      "2.0      51457\n",
      "4.0      49323\n",
      "70.0     43450\n",
      "99.0     40094\n",
      "Name: count, dtype: int64\n",
      "Top 5 secondary contributing factors:\n",
      "CFP2\n",
      "4.0     6288\n",
      "90.0    5850\n",
      "2.0     4313\n",
      "68.0    3900\n",
      "74.0    3767\n",
      "Name: count, dtype: int64\n",
      "Top 5 tertiary contributing factors:\n",
      "CFP3\n",
      "62.0    1084\n",
      "90.0     976\n",
      "68.0     852\n",
      "72.0     730\n",
      "75.0     610\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 7: Most common primary, secondary, and tertiary contributing factors\n",
    "\n",
    "# Most common primary contributing factor\n",
    "primary_factors = merged_df['CFP1'].value_counts().head(5)\n",
    "print(\"Top 5 primary contributing factors:\")\n",
    "print(primary_factors)\n",
    "\n",
    "# Most common secondary contributing factor\n",
    "secondary_factors = merged_df['CFP2'].value_counts().head(5)\n",
    "print(\"Top 5 secondary contributing factors:\")\n",
    "print(secondary_factors)\n",
    "\n",
    "# Most common tertiary contributing factor\n",
    "tertiary_factors = merged_df['CFP3'].value_counts().head(5)\n",
    "print(\"Top 5 tertiary contributing factors:\")\n",
    "print(tertiary_factors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Primary Contributing Factors\n",
    "- **1.0**: No Clear Contributing Factor\n",
    "- **2.0**: Fail to Yield Right of Way\n",
    "- **4.0**: Following Too Closely\n",
    "- **70.0**: Other Contributing Factor\n",
    "- **99.0**: Unknown\n",
    "\n",
    "### Top 5 Secondary Contributing Factors\n",
    "- **90.0**: Other Contributing Factor\n",
    "- **4.0**: Following Too Closely\n",
    "- **1.0**: No Clear Contributing Factor\n",
    "- **2.0**: Fail to Yield Right of Way\n",
    "- **74.0**: Other Human Contributing Factor\n",
    "\n",
    "### Top 5 Tertiary Contributing Factors\n",
    "- **62.0**: Weather\n",
    "- **90.0**: Other Contributing Factor\n",
    "- **72.0**: Driver Inattention/Distraction\n",
    "- **68.0**: Other Vision Related Factor\n",
    "- **75.0**: Driver on Phone/CB/Radio\n",
    "\n",
    "\n",
    "Intresting how phone is not the priamry reason for accidents it seems like or may be it is, just not represnetd that well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The part of the car that is most damaged is: 10.0\n",
      "Top 5 most damaged parts of the car:\n",
      "DAMAREA\n",
      "10.0    284119\n",
      "16.0    172146\n",
      "21.0     49743\n",
      "11.0     43936\n",
      "17.0     22750\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 8: What part of the car is the most damaged?\n",
    "most_damaged_part = merged_df['DAMAREA'].value_counts().idxmax()\n",
    "print(f\"The part of the car that is most damaged is: {most_damaged_part}\")\n",
    "\n",
    "# Also, let's print the top 5 most damaged parts\n",
    "top_5_damaged_parts = merged_df['DAMAREA'].value_counts().head(5)\n",
    "print(\"Top 5 most damaged parts of the car:\")\n",
    "print(top_5_damaged_parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Most Damaged Parts of the Car\n",
    "\n",
    "- **10.0**: Bottom (Undercarriage)\n",
    "- **16.0**: Multiple Areas\n",
    "- **21.0**: Other\n",
    "- **11.0**: Multiple Areas\n",
    "- **3.0**: Right Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common crash configuration (diagram) is: 12.0\n",
      "Top 5 most common crash configurations (diagrams):\n",
      "DIAGRAM\n",
      "12.0    316051\n",
      "5.0     158482\n",
      "10.0     74725\n",
      "13.0     31570\n",
      "90.0     29746\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 9: What is the usual crash configuration (diagram)?\n",
    "most_common_diagram = merged_df['DIAGRAM'].value_counts().idxmax()\n",
    "print(f\"The most common crash configuration (diagram) is: {most_common_diagram}\")\n",
    "\n",
    "# Also, let's print the top 5 most common crash configurations\n",
    "top_5_diagrams = merged_df['DIAGRAM'].value_counts().head(5)\n",
    "print(\"Top 5 most common crash configurations (diagrams):\")\n",
    "print(top_5_diagrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Most Common Crash Configurations (Diagrams)\n",
    "\n",
    "- **12.0**: Unknown (Not listed in the provided codes)\n",
    "- **5.0**: Right Angle\n",
    "- **10.0**: Head-On\n",
    "- **13.0**: Unknown (Not listed in the provided codes)\n",
    "- **90.0**: Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The deaths are: NUMFAT\n",
      "0    730847\n",
      "1      3155\n",
      "2       376\n",
      "3       118\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "max_deaths = merged_df['NUMFAT'].value_counts()\n",
    "print(f\"The deaths are: {max_deaths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Failed to yeild wrong way, find hotspots where the labeling is bad\n",
    "2. See alchoool deaths vs alchool tests vs all tests, are there roads athat alcholics tend to frequent\n",
    "3. winter accidents vs regular accidents - does the severtiy change for some roads, like roads that are very slippry in winter\n",
    "4. Sunrise vs sunsets, roads and times of years where the sune will be your enemy while driving \n",
    "5. How many times are the same poeple represented - what's the most number of times a person has been in an accident (not sure if we can do this)\n",
    "6. Different routing algorithms for summer vs winter, safe routs vs fast routes, etc. \n",
    "\n",
    "# ideas for safe routes\n",
    "\n",
    "Ways to approach it - \n",
    "\n",
    "1. Create a dictionary of clusters of accidents and times amd weatehr events where they are the most represented, and than use those to basically block certian routes. Than find the fastest road from the blocked routes.\n",
    "\n",
    "So basically the way to find the safest routes, is use the gmap api, but just create blocks on certain parts of the roads wehre a lot of accidents happen at certain time of day and times of year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
