{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do\n",
    "\n",
    "Now is a good time to look at everythign we have - here is the file location:\n",
    "\n",
    "/Users/yd211/Documents/GitHub/DataSet/DVSCrashData/MN Crash Data 2015 and Documentation/~$-Crash-Data-Code-Guide-2003-To-2015.docx\n",
    "\n",
    "\n",
    "Okay so here are somethings that we can do - some questions we can do:\n",
    "\n",
    "1. What age is the most represented, what age is the most over represented\n",
    "\n",
    "2. What ACC TIME bucket is the most prone? Challenge, waht ACC Time is the most overprone?\n",
    "\n",
    "3. What city has the most accidents?\n",
    "\n",
    "6. What percentage of accidents are given the alchool test?\n",
    "\n",
    "7. What are the most common primary secondar and tertiarry contributin factors and how represented are they (like what perecentage is the number one factor and number two factor) - could help in picking a direction for what to do to help reduce crashes the most\n",
    "\n",
    "8. What part of the car is the most damaged?\n",
    "\n",
    "9. What is the ususal crash configuration? - what disagram?\n",
    "\n",
    "10. At what kind of light do crashses usually occour, is it really true that nightime crashes are more often? Are they more dangerous? Is the speed higher at one timezone compared to other?\n",
    "\n",
    "11. Any make overrepresented in crashes?\n",
    "\n",
    "12. what's the maximum number of deaths in a crash?\n",
    "\n",
    "13. Do more alcholics crash or non alcholics?\n",
    "\n",
    "14. Suset vs Sunrise crashes and roadway direction - any changes in that?\n",
    "\n",
    "15. Any vehical types that are overrepresented?\n",
    "\n",
    "16. Any changes when snowing - WEATHER WEATH2 use that to track for snow and rain - what percentage of MN crashes occour in snow and are they more dangeorous than non snow - in terms of accident severatiy\n",
    "\n",
    "\n",
    "Other cool things that could be done reagrading map analaysis - \n",
    "\n",
    "1. Track all the accidents that occour when drinking alchool - roads that people who drink alchool usually take or is over represented\n",
    "\n",
    "2. Do analysis to find locations that have more accidents in snow than other weather events, might tell us a bit more about roads and intersections that need more work\n",
    "\n",
    "3. Could there be a mapping algorithm uses for weather (and possibly for drunk drivers)?\n",
    "\n",
    "\n",
    "Other observations - we get about 80,000 accidents each year accept 2020, where it was 50k - so total accidents that we have access to is around 450k or around half a million data points. That's a lot. We can also request data for 2022 and 2023 soon and add around 150k more accidents to our dataset. \n",
    "\n",
    "Population of MN is about 5.5 Million and has been increasing, perhaps we can do a population vs accients metric to see if the percentageg of people who get into an accident is consistent or not. Perhaps it will be. \n",
    "\n",
    "80k/5.5m = =8/550 or ~= 1.5%\n",
    "\n",
    "It would also be intresting to see how this figure compares against different states and such\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in merged data: 1983168\n",
      "Columns in the merged data: ['ACCN', 'AGENCY', 'LOCCASE', 'HITRUN', 'PROPDAM', 'NUMMV', 'NUMFAT', 'NUMINJ', 'DOLMIN', 'ACCDATE', 'COUNTY', 'CITYTWP', 'CITYNAME', 'ACCTYPE', 'SBUS', 'LOCFHE', 'BRIDGE', 'WKZNTYPE', 'LOCWKZN', 'WORKERS', 'RDSURF', 'INTREL', 'WEATHER1', 'WEATHER2', 'LIGHT', 'DIAGRAM', 'OFFTYPE', 'INTERSECT', 'ACCSEV', 'CFR1', 'CFR2', 'FATAL', 'FATWKZN', 'INJURY', 'INTYPE', 'LANDOWN', 'LEPRES', 'NUMNM', 'ONROAD', 'RTSYS', 'WKZNREL', 'CITY', 'XCOORD', 'YCOORD', 'TOWNSHIP', 'ACCTIME', 'URBRURT', 'ACCDAY', 'ACCYEAR', 'RVN_x', 'nmaction', 'unitper', 'RPN', 'POSITN', 'DLSTATE', 'DLCLASS', 'DLSTAT', 'VIOLS', 'DLREST1', 'PHYSCND1', 'RECOMND', 'SEX', 'SAFEQP1', 'AIRBAG', 'EJECT', 'INJSEV', 'ALCTEST', 'ALCTYPE', 'DRUGTEST', 'DRUGTYPE', 'METHHOSP', 'DLZIP', 'PTYPE', 'DISTRACT', 'ALCSUSP', 'NEWBAC', 'SPEEDING', 'CFP1', 'CFP2', 'CFP3', 'CFP4', 'CHARGED', 'DLENDOR1', 'DLENDOR2', 'DLENDOR3', 'DLJURIS', 'DLREST2', 'DLREST3', 'DRUGRES', 'DRUGSUSP', 'NMLOCTN', 'PHYSCND2', 'SAFEQP2', 'AGE', 'RVN_y', 'FIRE', 'TOWAWAY', 'TRAILER', 'DIRECTN', 'VEHMAKE', 'VEHMODEL', 'VEHYEAR', 'VEHCOLOR', 'REGSTATE', 'REGYEAR', 'EVENT1', 'EVENT2', 'EVENT3', 'EVENT4', 'MOSTHE', 'CFV1', 'CFV2', 'ACTION', 'TOTOCC', 'VEHTYPE', 'VEHUSE', 'DAMAREA', 'DAMSEV', 'CARGOTP', 'HAZPLAC', 'WAIVED', 'RDDES', 'RDALIGN', 'RDGRADE', 'TRFCNTL', 'WORKING', 'SPEED', 'UNITVEH', 'BUSTYPE', 'CMV', 'CMVCONFIG', 'CMVCNTRY', 'CMVSTATE', 'CMVTYPE', 'DRCONTRL', 'EMGUSE', 'GVWR', 'HAZREL', 'HITRUNV', 'INS', 'LANES', 'PARKED', 'UNITFAT', 'UNITINJ', 'VIN', 'INCIDENTID', 'Utmx', 'Utmy', 'UTMX', 'UTMY']\n",
      "Merged data saved to merged_data_2016_2017_2018_2019_2020_2021.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to load the data for a given year\n",
    "def load_data(year):\n",
    "    # base_path = f\"/Users/yd211/Documents/GitHub/DataSet/DVSCrashData/MN Crash Data {year}\"\n",
    "    base_path = rf\"C:\\Users\\yashd\\OneDrive\\Desktop\\Github Stuff\\DataSet\\DVSCrashData\\MN Crash Data {year}\"\n",
    "\n",
    "    base_path = fr\"C:\\Users\\prana\\Documents\\DataSet\\DataSet\\DVSCrashData\\MN Crash Data {year}\"\n",
    "    \n",
    "    # Define file paths\n",
    "    acc_loc = f\"{base_path}/mn-{year}-acc.txt\"\n",
    "    per_loc = f\"{base_path}/mn-{year}-per.txt\"\n",
    "    veh_loc = f\"{base_path}/mn-{year}-veh.txt\"\n",
    "    \n",
    "    # Read the data files\n",
    "    # We use low_memory=False to avoid the dtype warning for mixed types\n",
    "    accidents = pd.read_csv(acc_loc, delimiter='\\t', low_memory=False, encoding='latin1')\n",
    "    people = pd.read_csv(per_loc, delimiter='\\t', low_memory=False, encoding='latin1')\n",
    "    vehicles = pd.read_csv(veh_loc, delimiter='\\t', low_memory=False, encoding='latin1')\n",
    "    \n",
    "    return accidents, people, vehicles\n",
    "\n",
    "# Function to merge dataframes for a given year\n",
    "def merge_data(accidents, people, vehicles):\n",
    "    # Merge dataframes on 'ACCN' column\n",
    "    merged_df = pd.merge(accidents, people, on='ACCN', how='outer')\n",
    "    merged_df = pd.merge(merged_df, vehicles, on='ACCN', how='outer')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Function to load and merge data for multiple years\n",
    "def load_and_merge_years(years):\n",
    "    merged_yearly_data = []\n",
    "\n",
    "    for year in years:\n",
    "        accidents, people, vehicles = load_data(year)\n",
    "        merged_df = merge_data(accidents, people, vehicles)\n",
    "        merged_yearly_data.append(merged_df)\n",
    "    \n",
    "    # Concatenate all yearly dataframes\n",
    "    all_years_merged_df = pd.concat(merged_yearly_data, ignore_index=True)\n",
    "    return all_years_merged_df\n",
    "\n",
    "# Main function\n",
    "def main(years):\n",
    "    # Load and merge data for the given years\n",
    "    merged_df = load_and_merge_years(years)\n",
    "    \n",
    "    # Print some basic info\n",
    "    print(\"Number of rows in merged data:\", len(merged_df))\n",
    "    print(\"Columns in the merged data:\", merged_df.columns.tolist())\n",
    "\n",
    "    # Save merged data to a CSV file\n",
    "    years_str = \"_\".join(map(str, years))\n",
    "    merged_df.to_csv(f\"merged_data.csv\", index=False)\n",
    "    print(f\"Merged data saved to merged_data_{years_str}.csv\")\n",
    "\n",
    "\n",
    "years = [2016,2017,2018,2019,2020,2021]  # Change this to the list of years you want to analyze\n",
    "years = [2016, 2017, 2020, 2021]  # Change this to the list of years you want to analyze\n",
    "main(years)\n",
    "years_str = \"_\".join(map(str, years))\n",
    "CSV_path = \"merged_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.read_csv(CSV_path, low_memory=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most represented ages:\n",
      "AGE\n",
      "18.0    51761\n",
      "17.0    50908\n",
      "19.0    49671\n",
      "20.0    47987\n",
      "21.0    46758\n",
      "22.0    46571\n",
      "23.0    46215\n",
      "16.0    45634\n",
      "24.0    44973\n",
      "25.0    43074\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter out invalid ages (e.g., 999)\n",
    "valid_ages = merged_df[merged_df['AGE'] != 999]\n",
    "\n",
    "# Get the top 10 most represented ages\n",
    "top_10_ages = valid_ages['AGE'].value_counts().head(10)\n",
    "print(\"Top 10 most represented ages:\")\n",
    "print(top_10_ages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most represented ACC TIME values:\n",
      "ACCTIME\n",
      "1700    11739\n",
      "1600    11289\n",
      "1730     9908\n",
      "1500     9775\n",
      "1530     9558\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 most represented ACC TIME values\n",
    "top_5_times = merged_df['ACCTIME'].value_counts().head(5)\n",
    "print(\"Top 5 most represented ACC TIME values:\")\n",
    "print(top_5_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The city with the most accidents is: 2585\n",
      "Top 5 cities with the most accidents:\n",
      "CITY\n",
      "2585    282907\n",
      "3425    137889\n",
      "3235     49760\n",
      "3380     47051\n",
      "1040     45687\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 3: What city has the most accidents?\n",
    "most_accidents_city = merged_df['CITY'].value_counts().idxmax()\n",
    "print(f\"The city with the most accidents is: {most_accidents_city}\")\n",
    "\n",
    "# Also, let's print the top 5 cities with the most accidents\n",
    "top_5_cities = merged_df['CITY'].value_counts().head(5)\n",
    "print(\"Top 5 cities with the most accidents:\")\n",
    "print(top_5_cities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2585= Minneapolis\n",
    "\n",
    "3425=ST PAUL\n",
    "\n",
    "1040 = DULUTH \n",
    "\n",
    "3235 = ROCHESTER\n",
    "\n",
    "3380 = ST CLOUD \n",
    "\n",
    "Intresting to see Minneapolist and St.Paul as no 1 no 2, it's validating for sure. Another intresting thing we could do is divide this number by population and see what data we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of accidents where an alcohol test was given to at least one person is: 2.47%\n"
     ]
    }
   ],
   "source": [
    "# Question 6: What percentage of accidents are given the alcohol test?\n",
    "\n",
    "# Filter out the rows where ALCTEST is relevant (i.e., rows related to people)\n",
    "people_with_alcohol_test_info = merged_df.dropna(subset=['ALCTEST'])\n",
    "\n",
    "# Calculate the percentage of people given an alcohol test\n",
    "alcohol_test_given = people_with_alcohol_test_info[people_with_alcohol_test_info['ALCTEST'] == 1]\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_alcohol_test_given = (len(alcohol_test_given) / len(people_with_alcohol_test_info)) * 100\n",
    "print(f\"The percentage of accidents where an alcohol test was given to at least one person is: {percentage_alcohol_test_given:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 primary contributing factors:\n",
      "CFP1\n",
      "1.0     1032720\n",
      "2.0      139932\n",
      "4.0      131288\n",
      "99.0     126401\n",
      "70.0     102224\n",
      "Name: count, dtype: int64\n",
      "Top 5 secondary contributing factors:\n",
      "CFP2\n",
      "90.0    16163\n",
      "4.0     15819\n",
      "2.0     11872\n",
      "68.0    11254\n",
      "70.0    10508\n",
      "Name: count, dtype: int64\n",
      "Top 5 tertiary contributing factors:\n",
      "CFP3\n",
      "90.0    3237\n",
      "62.0    3101\n",
      "68.0    2405\n",
      "72.0    2085\n",
      "70.0    1839\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 7: Most common primary, secondary, and tertiary contributing factors\n",
    "\n",
    "# Most common primary contributing factor\n",
    "primary_factors = merged_df['CFP1'].value_counts().head(5)\n",
    "print(\"Top 5 primary contributing factors:\")\n",
    "print(primary_factors)\n",
    "\n",
    "# Most common secondary contributing factor\n",
    "secondary_factors = merged_df['CFP2'].value_counts().head(5)\n",
    "print(\"Top 5 secondary contributing factors:\")\n",
    "print(secondary_factors)\n",
    "\n",
    "# Most common tertiary contributing factor\n",
    "tertiary_factors = merged_df['CFP3'].value_counts().head(5)\n",
    "print(\"Top 5 tertiary contributing factors:\")\n",
    "print(tertiary_factors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Primary Contributing Factors\n",
    "- **1.0**: No Clear Contributing Factor\n",
    "- **2.0**: Fail to Yield Right of Way\n",
    "- **4.0**: Following Too Closely\n",
    "- **70.0**: Other Contributing Factor\n",
    "- **99.0**: Unknown\n",
    "\n",
    "### Top 5 Secondary Contributing Factors\n",
    "- **90.0**: Other Contributing Factor\n",
    "- **4.0**: Following Too Closely\n",
    "- **1.0**: No Clear Contributing Factor\n",
    "- **2.0**: Fail to Yield Right of Way\n",
    "- **74.0**: Other Human Contributing Factor\n",
    "\n",
    "### Top 5 Tertiary Contributing Factors\n",
    "- **62.0**: Weather\n",
    "- **90.0**: Other Contributing Factor\n",
    "- **72.0**: Driver Inattention/Distraction\n",
    "- **68.0**: Other Vision Related Factor\n",
    "- **75.0**: Driver on Phone/CB/Radio\n",
    "\n",
    "\n",
    "Intresting how phone is not the priamry reason for accidents it seems like or may be it is, just not represnetd that well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The part of the car that is most damaged is: 10.0\n",
      "Top 5 most damaged parts of the car:\n",
      "DAMAREA\n",
      "10.0    781089\n",
      "16.0    446853\n",
      "21.0    137247\n",
      "11.0    118340\n",
      "17.0     61456\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 8: What part of the car is the most often damaged?\n",
    "most_damaged_part = merged_df['DAMAREA'].value_counts().idxmax()\n",
    "print(f\"The part of the car that is most damaged is: {most_damaged_part}\")\n",
    "\n",
    "# Also, let's print the top 5 most damaged parts\n",
    "top_5_damaged_parts = merged_df['DAMAREA'].value_counts().head(5)\n",
    "print(\"Top 5 most damaged parts of the car:\")\n",
    "print(top_5_damaged_parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Most Damaged Parts of the Car\n",
    "\n",
    "- **10.0**: Bottom (Undercarriage)\n",
    "- **16.0**: Multiple Areas\n",
    "- **21.0**: Other\n",
    "- **11.0**: Multiple Areas\n",
    "- **3.0**: Right Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common crash configuration (diagram) is: 12.0\n",
      "Top 5 most common crash configurations (diagrams):\n",
      "DIAGRAM\n",
      "12.0    818869\n",
      "5.0     455487\n",
      "10.0    206199\n",
      "13.0     90251\n",
      "90.0     72409\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 9: What is the usual crash configuration (diagram)?\n",
    "most_common_diagram = merged_df['DIAGRAM'].value_counts().idxmax()\n",
    "print(f\"The most common crash configuration (diagram) is: {most_common_diagram}\")\n",
    "\n",
    "# Also, let's print the top 5 most common crash configurations\n",
    "top_5_diagrams = merged_df['DIAGRAM'].value_counts().head(5)\n",
    "print(\"Top 5 most common crash configurations (diagrams):\")\n",
    "print(top_5_diagrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Most Common Crash Configurations (Diagrams)\n",
    "\n",
    "- **12.0**: Unknown (Not listed in the provided codes)\n",
    "- **5.0**: Right Angle\n",
    "- **10.0**: Head-On\n",
    "- **13.0**: Unknown (Not listed in the provided codes)\n",
    "- **90.0**: Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The deaths are: NUMFAT\n",
      "0    1972194\n",
      "1       9722\n",
      "2       1043\n",
      "3        171\n",
      "4         26\n",
      "6         12\n",
      "Name: count, dtype: int64\n",
      "the percent of accidents that result in deaths = 0.6301533707683867\n"
     ]
    }
   ],
   "source": [
    "max_deaths = merged_df['NUMFAT'].value_counts()\n",
    "print(f\"The deaths are: {max_deaths}\")\n",
    "\n",
    "print(\"the percent of accidents that result in deaths = \" + str(sum(merged_df['NUMFAT'].values)/merged_df.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damn, almost half a percent of all the accidents result in deaths - that's kinda high still \n",
    "\n",
    "1. Failed to yeild wrong way, find hotspots where the labeling is bad\n",
    "2. See alchoool deaths vs alchool tests vs all tests, are there roads athat alcholics tend to frequent\n",
    "3. winter accidents vs regular accidents - does the severtiy change for some roads, like roads that are very slippry in winter\n",
    "4. Sunrise vs sunsets, roads and times of years where the sune will be your enemy while driving \n",
    "5. How many times are the same poeple represented - what's the most number of times a person has been in an accident (not sure if we can do this)\n",
    "6. Different routing algorithms for summer vs winter, safe routs vs fast routes, etc. \n",
    "\n",
    "# ideas for safe routes\n",
    "\n",
    "Ways to approach it - \n",
    "\n",
    "1. Create a dictionary of clusters of accidents and times amd weatehr events where they are the most represented, and than use those to basically block certian routes. Than find the fastest road from the blocked routes.\n",
    "\n",
    "So basically the way to find the safest routes, is use the gmap api, but just create blocks on certain parts of the roads wehre a lot of accidents happen at certain time of day and times of year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather: WEATHER1\n",
      "1     1284553\n",
      "2      402249\n",
      "4      139884\n",
      "3       99680\n",
      "5       17428\n",
      "7       16415\n",
      "99      13355\n",
      "6        6313\n",
      "90       2422\n",
      "8         869\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "weather_acc = merged_df['WEATHER1'].value_counts()\n",
    "print(f\"Weather: {weather_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key for the wather stuff\n",
    "\n",
    "\n",
    "1=CLEAR\n",
    "\n",
    "2=CLOUDY \n",
    "\n",
    "3=RAIN \n",
    "\n",
    "4=SNOW \n",
    "\n",
    "5=SLT/HAIL/FRZ RN\n",
    "\n",
    "6=FOG/SMOG/SMOKE \n",
    "\n",
    "7=BLWNG SND/DS/SN\n",
    "\n",
    "8=SEVERE CROSSWNDS \n",
    "\n",
    "90=OTHR WEATHER \n",
    "\n",
    "99=UNKNWN WEATHER \n",
    "\n",
    "00=LEFT BLANK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of accidents that occour in snow = 5.026301352179947\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent of accidents that occour in snow = \" + str(weather_acc.values[3]/sum(weather_acc.values)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of fatalities that occurred during snow: 5.473313595262863%\n"
     ]
    }
   ],
   "source": [
    "fatalities_df = merged_df[merged_df['NUMFAT'] > 0]\n",
    "\n",
    "# Count the number of deaths that occurred during snow (WEATHER1 == 4)\n",
    "snow_fatalities_count = fatalities_df[fatalities_df['WEATHER1'] == 4]['NUMFAT'].sum()\n",
    "\n",
    "# Calculate the percentage of total deaths that happened during snow\n",
    "percent_snow_fatalities = (snow_fatalities_count / sum(fatalities_df['NUMFAT'])) * 100\n",
    "\n",
    "print(f\"Percentage of fatalities that occurred during snow: {percent_snow_fatalities}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow so you are less likely to die when it's snowing vs when it's not snowing -- fascinating. I would have not anticipated that - you go from 5.57 to 2.87 (at least for the year 2016)\n",
    "\n",
    "Well I guess that's not as prevelent when you account for all the years. When you do account for all the year - it turns out that there is actually an uptick in snow accidents when it's snowing in comparsion to when it's not - around a 10% uptick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most represented ACC TIME values:\n",
      "ACCTIME\n",
      "445     189\n",
      "1530    118\n",
      "1736    107\n",
      "1900     82\n",
      "914      74\n",
      "Name: count, dtype: int64\n",
      "Top 10 most represented ages:\n",
      "AGE\n",
      "19.0    247\n",
      "21.0    238\n",
      "22.0    234\n",
      "23.0    227\n",
      "18.0    218\n",
      "26.0    216\n",
      "30.0    205\n",
      "24.0    203\n",
      "56.0    195\n",
      "29.0    194\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Snow accident Time and Age Stuff\n",
    "\n",
    "# Print the top 5 most represented ACC TIME values\n",
    "top_5_snowtimes = fatalities_df['ACCTIME'].value_counts().head(5)\n",
    "print(\"Top 5 most represented ACC TIME values:\")\n",
    "print(top_5_snowtimes)\n",
    "\n",
    "# Filter out invalid ages (e.g., 999)\n",
    "valid_snowages = fatalities_df[fatalities_df['AGE'] != 999]\n",
    "\n",
    "# Get the top 10 most represented ages\n",
    "top_10_ages = valid_snowages['AGE'].value_counts().head(10)\n",
    "print(\"Top 10 most represented ages:\")\n",
    "print(top_10_ages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the year 2016, it was definetey intresting to see how 40 was the most represented age, and it was more spread out, but over the years I guess it flattens out and 19 does end up coming at the top. I guess 2016 was a weird year for snow. Which is kind of intresting. Year by year varations still definetly exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VEHTYPE\n",
      "2.0     995742\n",
      "4.0     487695\n",
      "3.0     219218\n",
      "5.0      93417\n",
      "49.0     57857\n",
      "52.0     23224\n",
      "53.0     14887\n",
      "31.0     12136\n",
      "6.0      12133\n",
      "48.0     11761\n",
      "13.0     11648\n",
      "90.0      8990\n",
      "54.0      8422\n",
      "14.0      5903\n",
      "50.0      1556\n",
      "51.0      1076\n",
      "20.0       915\n",
      "22.0       862\n",
      "32.0       786\n",
      "15.0       569\n",
      "10.0       313\n",
      "21.0       169\n",
      "33.0       159\n",
      "34.0        92\n",
      "70.0        20\n",
      "Name: count, dtype: int64\n",
      "\n",
      " \n",
      " \n",
      "VEHTYPE\n",
      "2.0     3760\n",
      "4.0     2092\n",
      "3.0     1553\n",
      "53.0     905\n",
      "49.0     841\n",
      "31.0     752\n",
      "5.0      446\n",
      "54.0     131\n",
      "22.0      99\n",
      "52.0      65\n",
      "48.0      45\n",
      "50.0      43\n",
      "14.0      37\n",
      "13.0      35\n",
      "90.0      32\n",
      "6.0       30\n",
      "15.0      26\n",
      "32.0      20\n",
      "51.0      16\n",
      "21.0      12\n",
      "20.0      10\n",
      "34.0       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['VEHTYPE'].value_counts())\n",
    "\n",
    "print(\"\\n \\n \")\n",
    "\n",
    "print(fatalities_df['VEHTYPE'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we can do with this data\n",
    "\n",
    "1. Identify road clusters that have a lot of accidents - find accident prone areas\n",
    "\n",
    "    1. Try to find accdient prone time zones in accident prone areas, perhaps accident prone weather zones\n",
    "\n",
    "2. Genetic Algorithms\n",
    "\n",
    "3. decoder archetecture with locations being the nodes -- encode accident data as the main cost optimizer to minimize\n",
    "\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is just for demo purposes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
